{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) A common problem in linear algebra is solving linear equations. Consider the following linear equations:\n",
    "    \n",
    "    [2x + y - z = 8]\n",
    "    [-3x - y + 2z = -11]\n",
    "    [ -2x + y + 2z = -3]\n",
    "    \n",
    "If we let:\n",
    "\n",
    "$A = \\begin{pmatrix}    2  & 1  & -1 \\\\ \n",
    "                       -3 & -1 & 2  \\\\    \n",
    "                       -2 & 1  & 2 \\end{pmatrix}$\n",
    "                 \n",
    "                 \n",
    "$\\qquad \\mathbf{x} = \\begin{pmatrix} x \\\\ \n",
    "                                        y  \\\\ \n",
    "                                        z  \\end{pmatrix}$\n",
    "                                        \n",
    "                                        \n",
    "$\\qquad \\mathbf{b} = \\begin{pmatrix} 8 \\\\\n",
    "                                      -11 \\\\ \n",
    "                                      -3 \\end{pmatrix}$\n",
    "\n",
    "Then the linear equations above can be written as $A\\mathbf{x} = \\mathbf{b}$\n",
    "\n",
    "Write a function that can return the vector **x**, given the matrix **A** and the vector **b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_lin_eq(A, b):\n",
    "    '''\n",
    "    u: a matrix\n",
    "    x: a vector\n",
    "    returns: the vector that maps u to x\n",
    "    '''\n",
    "    return np.linalg.inv(A).dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "u = np.array([[3,3,3],[-3,-5,2],[-10,200,10]])\n",
    "x = np.array([[3],[-11],[-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.34954128],\n",
       "       [ 0.17889908],\n",
       "       [-1.52844037]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_lin_eq(u,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.) Can this solve any linear equation? Can you find an example of a linear equation that it cannot solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If the columns of the matrix of coefficients are not linearly independent, no unique vector (x, y, z, ...) will be found. Thus, we need our matrix of coefficients (A) to be invertible. Consider the following system of equations:\n",
    "\n",
    "```\n",
    "  x +  3y + ?z = 14\n",
    "14x -   y + ?z = 30\n",
    "-3x + 12y + ?z =  5\n",
    "```\n",
    "\n",
    "Replace the column for z's coefficients with any linear combination of x and y. Here, I will let col(z) = 1 \\* col(x) + 2 \\* col(y)\n",
    "\n",
    "```\n",
    "  x +  3y +  7z = 14\n",
    "14x -   y + 12z = 30\n",
    "-3x + 12y + 21z = 5\n",
    "\n",
    "-> A = ( 1  3  7)\n",
    "       (14 -1 12)\n",
    "       (-3 12 21)\n",
    "```\n",
    "\n",
    "From this, we can calculate the det or RREF to see if the columns of the matrix are linearly independent:\n",
    "\n",
    "```\n",
    "det(A) = 0\n",
    "rref(A) = (1 0 1)\n",
    "          (0 1 2)\n",
    "          (0 0 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.) How can something like this be used in a machine learning or deep learning task? What might the matrix **A** be and what might the vector **b** represent in that case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given layer, we can find the activations for that layer if we know the activations of the previous layer (b) and the weights of the connections from the previous layer to the current layer (A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Watch the 9-minute YouTube video below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/b269qpILOpk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/b269qpILOpk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) Answer the following (by description, not math):\n",
    "\n",
    "* What is a vector? Where do vectors start and end?\n",
    "* What is a subspace?\n",
    "* What is a projection?\n",
    "* How can one find the vector v that is the projection of x onto the subspace?\n",
    "* What does \"orthogonal\" mean? Why don't we just say \"perpendicular\" instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A vector is a straight line in space. Generally, vectors are thought to start at the origin. However, when doing vector arithmetic, we put them head to tail. Eg: vec(c) = vec(a) + vec(b) means that vector c equals the same end-point as vectors a and b, put head to tail, if both c and a have the same start point.\n",
    "* A subspace in R\\_n is the set of all points defined a number of linearly independent vectors v\\_1, v\\_2, ..., v\\_m with m < n. In essence, a sub-space is some version of a space of a lower dimension inside a higher dimension. A subspace in R\\_3 defined by two linearly independent vectors is a plane, since R\\_2 is a plane.\n",
    "* A projection of a vector onto a subspace is a version of that vector that lives in the subspace. It is a shadow of the vector. It is the vector in the subspace with the a distance to the original vector that is less than or equal to any other vector in the subspace.\n",
    "* Based on the Khan Academy video, it looks like v + a = x, where v is the orthogonal projection of vector x onto the subspace, and a is some other unknown vector that is orthogonal to the subspace. This can be rearranged as v = x - a. If we can find the projection of x onto a, we can subtract it from x to find the projection. Note that the following strategy only works in 3-space where the subspace is a 2-space. We find the cross-product of the two basis vectors to find the orthogonal, call it o, then we project x into it in the following way: $ v = {\\frac{x \\cdot o}{o \\cdot o}o} $. There is a more generalized form, though. a is orthogonal to the subspace and is the only vector orthogonal to the subspace. We can think of it as removing the vectors orthogonal to the subspace or, in other words, a summation of all the orthogonal vectors inside the subspace. We can use the Gram-Schmidt Process to transform a set of basis vectors for a subspace into a set of orthogonal basis vectors for that subspace. Then, we simply find the projection of the vector x onto each of the orthogonal basis vectors, then add them all up to find the projection.\n",
    "* The word perpendicular is generally used in 2-space. In 2-space, for any line, there is exactly one perpendicular line. However, we use the word orthogonal in higher dimensions. Orthogonal seems, to me, to be a more general term. We can talk about a vector being orthogonal to one or more vectors, whereas \"perpendicular\" tends to be used to mean \"orthogonal to one vector.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6). Click on the link to read the PDF and answer the questions that follow. \n",
    "\n",
    "* http://www.deeplearningbook.org/linear_algebra.pdf\n",
    "* Given the above reading and the video in question 4, write a function that, given a vector (a 2D line) and a point **x**, can return the point on on that vector/line **y** that is the closest point on that line to the point. \n",
    "* Hints: it might help to try to plot the vector and the line between **x** and **y** (make sure the scale of the axes are the same). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec = [8.643773428593772, 8.92024822369327, 4.443959242562721, 1.3499461135631319]\n",
      "pt = [8.812563711794853, 7.076605175080909, 2.81402365058089, 1.0483022507655437]\n",
      "result = [7.53109507 7.77198037 3.87190615 1.17617295]\n"
     ]
    }
   ],
   "source": [
    "def proj(vector, x):\n",
    "    # Make unit vector:\n",
    "    vec_len = 0\n",
    "    for v in vector:\n",
    "        vec_len  = vec_len + np.power(v, 2)\n",
    "    vec_len = np.sqrt(vec_len)\n",
    "    u = np.divide(vector, vec_len)\n",
    "    # x^T = [ x1, x2, ... ]\n",
    "    # x^T u = [ x1*u1, x2*u2, ... ]\n",
    "    # This is just the dot product, and we can represent the point x as another vector.\n",
    "    return np.multiply(np.dot(x, u), u)\n",
    "\n",
    "# Test below. Matches Wolfram Alpha's result.\n",
    "vec = [ 8.643773428593772, 8.92024822369327, 4.443959242562721, 1.3499461135631319 ]\n",
    "pt = [ 8.812563711794853, 7.076605175080909, 2.81402365058089, 1.0483022507655437 ]\n",
    "print('vec = ' + str(vec))\n",
    "print('pt = ' + str(pt))\n",
    "result = proj(vec, pt)\n",
    "print('result = ' + str(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
